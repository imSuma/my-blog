---
title: How I Built a PDF Chat App with WebLLM
date: 2025/1/10
description: A step-by-step guide to building a browser-based PDF chat application using React and WebLLM for local AI processing
tag: AI, WebLLM, React, PDF processing, tutorial
author: Developer
---

# How I Built a PDF Chat App with WebLLM

**Live Demo:** [https://askpdf-offline.vercel.app/](https://askpdf-offline.vercel.app/)  
**Source Code:** [https://github.com/imSuma/askpdf-offline](https://github.com/imSuma/askpdf-offline)

I recently built a simple PDF chat application that runs completely in the browser. You can upload a PDF, and then ask questions about its content using AI - all without sending any data to external servers. Here's how I did it and what I learned along the way.

## What We're Building

The app has three main features:
- Upload and read PDF files in the browser
- Extract text from PDFs using PDF.js
- Chat with an AI model (Llama 3.2) about the PDF content

Everything runs locally in your browser, which means your documents stay private on your device.

## Getting Started

Before we dive into the code, let's set up the project. I used React with TypeScript and a few key libraries:

```bash
npm create vite@latest pdf-chat-app -- --template react-ts
cd pdf-chat-app
npm install @mlc-ai/web-llm pdfjs-dist
npm install -D @types/pdfjs-dist
```

The main libraries we'll use:
- `@mlc-ai/web-llm` - Runs AI models in the browser
- `pdfjs-dist` - Extracts text from PDF files

## Step 1: Setting Up PDF Processing

First, let's create a component to handle PDF uploads and text extraction:

```typescript
// PdfUpload.tsx
import { useState } from 'react';
import * as pdfjsLib from 'pdfjs-dist';

// Configure PDF.js worker
pdfjsLib.GlobalWorkerOptions.workerSrc = 
  'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';

export function PdfUpload({ onTextExtracted }: { onTextExtracted: (text: string) => void }) {
  const [isProcessing, setIsProcessing] = useState(false);

  const extractTextFromPdf = async (file: File) => {
    setIsProcessing(true);
    try {
      const arrayBuffer = await file.arrayBuffer();
      const pdf = await pdfjsLib.getDocument(arrayBuffer).promise;
      
      let fullText = '';
      
      // Extract text from each page
      for (let i = 1; i <= pdf.numPages; i++) {
        const page = await pdf.getPage(i);
        const textContent = await page.getTextContent();
        const pageText = textContent.items
          .map((item: any) => item.str)
          .join(' ');
        fullText += pageText + '\n';
      }
      
      onTextExtracted(fullText);
    } catch (error) {
      console.error('Error extracting PDF text:', error);
    } finally {
      setIsProcessing(false);
    }
  };

  return (
    <div className="border-2 border-dashed border-gray-300 p-6 text-center">
      <input
        type="file"
        accept=".pdf"
        onChange={(e) => {
          const file = e.target.files?.[0];
          if (file) extractTextFromPdf(file);
        }}
        className="mb-4"
      />
      {isProcessing && <p>Processing PDF...</p>}
    </div>
  );
}
```

This component handles file uploads and uses PDF.js to extract text from each page.

## Step 2: Setting Up WebLLM

Next, let's create a hook to manage the AI model:

```typescript
// useWebLLM.ts
import { useState, useRef } from 'react';
import * as webllm from '@mlc-ai/web-llm';

export function useWebLLM() {
  const [engine, setEngine] = useState<webllm.MLCEngine | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [isReady, setIsReady] = useState(false);
  const [progress, setProgress] = useState(0);

  const initEngine = async () => {
    if (engine) return;
    
    setIsLoading(true);
    try {
      const newEngine = await webllm.CreateMLCEngine(
        "Llama-3.2-3B-Instruct-q4f16_1-MLC",
        {
          initProgressCallback: (report) => {
            setProgress(Math.round(report.progress * 100));
          }
        }
      );
      setEngine(newEngine);
      setIsReady(true);
    } catch (error) {
      console.error('Failed to initialize WebLLM:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const chatWithPDF = async (message: string, pdfText: string) => {
    if (!engine) return '';
    
    const prompt = `Based on this document content:
    
${pdfText.substring(0, 3000)} // Limit context size

Question: ${message}

Please answer based only on the information in the document above.`;

    const response = await engine.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
      max_tokens: 500,
    });

    return response.choices[0]?.message?.content || '';
  };

  return { initEngine, chatWithPDF, isLoading, isReady, progress };
}
```

This hook handles model initialization and provides a simple interface for chatting with PDF content.

## Step 3: Bringing It All Together

Now let's create the main app component that combines everything:

```typescript
// App.tsx
import { useState } from 'react';
import { PdfUpload } from './PdfUpload';
import { useWebLLM } from './useWebLLM';

export default function App() {
  const [pdfText, setPdfText] = useState('');
  const [messages, setMessages] = useState<{ user: string; ai: string }[]>([]);
  const [userInput, setUserInput] = useState('');
  const [isThinking, setIsThinking] = useState(false);
  
  const { initEngine, chatWithPDF, isLoading, isReady, progress } = useWebLLM();

  const handleSendMessage = async () => {
    if (!userInput.trim() || !pdfText || !isReady) return;

    setIsThinking(true);
    const response = await chatWithPDF(userInput, pdfText);
    
    setMessages(prev => [...prev, { user: userInput, ai: response }]);
    setUserInput('');
    setIsThinking(false);
  };

  return (
    <div className="max-w-4xl mx-auto p-6">
      <h1 className="text-3xl font-bold mb-6">PDF Chat App</h1>
      
      {!pdfText ? (
        <PdfUpload onTextExtracted={setPdfText} />
      ) : !isReady ? (
        <div className="text-center">
          <button 
            onClick={initEngine}
            disabled={isLoading}
            className="bg-blue-500 text-white px-4 py-2 rounded"
          >
            {isLoading ? `Loading AI Model... ${progress}%` : 'Initialize AI'}
          </button>
        </div>
      ) : (
        <div>
          <div className="bg-gray-100 p-4 rounded mb-4 max-h-60 overflow-y-auto">
            <h3 className="font-semibold mb-2">Chat History:</h3>
            {messages.map((msg, i) => (
              <div key={i} className="mb-4">
                <div className="font-semibold">You: {msg.user}</div>
                <div className="text-gray-700">AI: {msg.ai}</div>
              </div>
            ))}
            {isThinking && <div className="text-gray-500">AI is thinking...</div>}
          </div>
          
          <div className="flex gap-2">
            <input
              type="text"
              value={userInput}
              onChange={(e) => setUserInput(e.target.value)}
              onKeyPress={(e) => e.key === 'Enter' && handleSendMessage()}
              placeholder="Ask a question about the PDF..."
              className="flex-1 p-2 border rounded"
            />
            <button
              onClick={handleSendMessage}
              disabled={!userInput.trim() || isThinking}
              className="bg-blue-500 text-white px-4 py-2 rounded disabled:opacity-50"
            >
              Send
            </button>
          </div>
        </div>
      )}
    </div>
  );
}
```

This creates a simple interface where users can upload a PDF, initialize the AI, and start chatting.

## Important Notes

A few things I learned while building this:

### Browser Compatibility
WebLLM requires a modern browser with WebAssembly support. I included a progress indicator because the model download can take 1-2 minutes on first load.

### Memory Usage
The Llama 3.2 3B model uses about 2-3GB of RAM. I chose this model because it's a good balance between performance and resource usage for browser-based applications.

### Context Limits
I limit the PDF content to 3000 characters when sending to the AI to avoid hitting token limits. For longer documents, you might want to implement chunking or semantic search.

## What's Next?

This is a basic implementation, but there are many ways to improve it:

- **Better text chunking** for large PDFs
- **Conversation memory** to remember previous questions
- **Multiple document support** 
- **Better error handling** for unsupported PDFs

## Try It Yourself

You can see the full working version at [askpdf-offline.vercel.app](https://askpdf-offline.vercel.app/) or check out the complete source code on [GitHub](https://github.com/imSuma/askpdf-offline).

The beauty of this approach is that everything runs in your browser - no servers, no API keys, and your documents never leave your device. It's a great example of how modern web technologies can create powerful, privacy-focused applications.